"use strict";(self.webpackChunkpersonal_site=self.webpackChunkpersonal_site||[]).push([[798],{9350:function(e,n){n.A=["#003547","#005E54","#C2BB00","#F24405","#ED8B16"]},9798:function(e,n,t){t.r(n),t.d(n,{default:function(){return ee}});var i=t(5544),a=t(4765),o=t(5043),r=t(4496),s=t(4605),c=t(3217),l=t(7470),u=t(6950),h=t(4992),d=t(6328),p=t(8321),g=t(675),m=t(467),f=t(1906),x=t(5874),b=t(7603),A=t(7353),v=t(8739),w=t(2578),y=t(2110),j=t(6494),k=t(279),C=t(4535),P=t(8293),I=t(6591),S=t(6490),T=t(3248),M=t(9347),W=t(9350),J=t(579),F="";function E(){return(E=(0,m.A)((0,g.A)().mark((function e(n){var t,i,a,o;return(0,g.A)().wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t=n.split("/").slice(-2).join("/"),e.prev=1,e.next=4,fetch("https://img.shields.io/github/stars/".concat(t));case 4:return i=e.sent,e.next=7,i.text();case 7:return a=e.sent,o=a.split("</text></a></g></svg>")[0].split(">")[31],e.abrupt("return",o);case 12:return e.prev=12,e.t0=e.catch(1),e.abrupt("return","0");case 15:case"end":return e.stop()}}),e,null,[[1,12]])})))).apply(this,arguments)}var L=(0,C.Ay)(r.A)((function(){return{fontSize:"28px",margin:"0.5em 0em 0.25em 0em",lineHeight:"160%",fontWeight:"400",letterSpacing:"-0.02em",fontFamily:"S\xf6hne, sans-serif",color:"#080808"}})),D=(0,C.Ay)(r.A)((function(){return{fontFamily:"Arial, sans-serif",margin:"20px",padding:"15px",borderRadius:"5px",whiteSpace:"pre-wrap"}})),O=(0,C.Ay)("strong")({color:"#1a1a1a",fontSize:"21px",fontWeight:"700"}),Y=(0,C.Ay)(I.A)((function(){return{flexShrink:0,width:"40%",marginLeft:"auto",marginRight:"auto",height:"auto"}})),G=(0,C.Ay)("sup")({fontSize:"0.8em",position:"relative",top:"-0.2em"}),z=function(e,n,t){var i=e.trim(),a="Jinghao Wang"===i||"Jinghao Wang*"===i,o=i.includes("#");return(0,J.jsxs)("span",{children:[a?(0,J.jsx)(O,{children:i}):o?(0,J.jsxs)(J.Fragment,{children:[i.replace("#",""),(0,J.jsx)(G,{children:"\u2709"})]}):i,t.length===n+1?"":",\xa0\xa0"]},n)};function Z(e){var n=e.data,t=e.expandAllAbstract,a=(0,o.useState)(!1),r=(0,i.A)(a,2),s=r[0],c=r[1],l=(0,o.useState)("0"),u=(0,i.A)(l,2),h=u[0],d=u[1],p=(0,o.useState)(!1),g=(0,i.A)(p,2),m=g[0],C=g[1];(0,o.useEffect)((function(){n.link.github&&function(e){return E.apply(this,arguments)}(n.link.github).then(d)}),[n.link.github]),(0,o.useEffect)((function(){c(t)}),[t]);var I=function(){return C((function(e){return!e}))},O=(0,o.useMemo)((function(){return n.author.split(",").map(z)}),[n.author]);return(0,J.jsxs)(y.A,{sx:{display:"flex",marginBottom:"1.5em"},children:[(0,J.jsx)(A.A,{sx:{display:"flex",flexDirection:"column"},children:(0,J.jsxs)(j.A,{children:[(0,J.jsx)(L,{sx:{textAlign:"center",fontSize:"26px",paddingX:"1em",marginBottom:"1em"},children:n.title}),(0,J.jsxs)(A.A,{sx:{display:"flex",flexDirection:"column",marginLeft:"2em"},children:[(0,J.jsx)(L,{sx:{fontSize:"18px"},children:O}),(0,J.jsx)(L,{sx:{fontSize:"18px",fontStyle:"italic"},children:n.conference}),(0,J.jsxs)(L,{variant:"body2",sx:{color:"#a0a0a0cc",fontSize:"18px"},children:["TL;DR: ",n.tldr]}),(0,J.jsx)(L,{sx:{fontSize:"16px"},children:n.time}),(0,J.jsxs)(A.A,{sx:{"& button":{marginRight:"1em",padding:"0"}},children:[n.bibtex&&(0,J.jsx)(f.A,{size:"small",endIcon:(0,J.jsx)(x.A,{}),onClick:function(){return window.open(n.link.paper)},sx:{color:W.A[0],textTransform:"none"},children:"Paper"}),n.link.github&&(0,J.jsx)(f.A,{size:"small",endIcon:(0,J.jsx)(b.A,{}),onClick:function(){return window.open(n.link.github)},sx:{color:W.A[1],textTransform:"none"},children:"Code"}),n.link.github&&(0,J.jsx)(v.A,{badgeContent:h,sx:{marginRight:"1em"},children:(0,J.jsx)(w.A,{sx:{color:W.A[2]}})}),n.bibtex&&(0,J.jsx)(f.A,{size:"small",endIcon:(0,J.jsx)(S.A,{}),onClick:I,sx:{color:W.A[3],textTransform:"none"},children:"BibTeX"}),(0,J.jsxs)(T.A,{open:m,onClose:I,children:[(0,J.jsx)(D,{children:n.bibtex}),(0,J.jsx)(M.A,{children:(0,J.jsx)(f.A,{autoFocus:!0,onClick:I,sx:{color:"#a0a0a0cc"},children:"Close"})})]}),(0,J.jsx)(f.A,{expand:s,onClick:function(){return c((function(e){return!e}))},sx:{color:W.A[4],textTransform:"none"},endIcon:(0,J.jsx)(P.A,{}),children:"Abstract"}),(0,J.jsx)(k.A,{in:s,timeout:"auto",unmountOnExit:!0,children:(0,J.jsx)(j.A,{children:(0,J.jsx)(L,{sx:{fontSize:"16px"},children:n.abstract})})})]})]})]})}),(0,J.jsx)(Y,{image:"".concat(F,"/painting/").concat(n.image_path)})]})}var R=[{title:"FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",author:"Yukang Cao, Chenyang Si, Jinghao Wang, Ziwei Liu#",conference:"under review",time:"2024-10",link:{},abstract:"We present FreeMorph, the first tuning-free method for image morphing that accommodates inputs with varying semantics or layouts. Unlike existing methods, which rely on fine-tuning pre-trained diffusion models and are limited by time constraints and semantic/layout discrepancies, FreeMorph delivers high-fidelity image morphing without extensive training. Despite its efficiency and potential, tuning-free methods still face challenges in maintaining high-quality image morphing due to the non-linear nature of the multi-step denoising process and bias inherited from the pre-trained diffusion model. In this paper, we introduce FreeMorph to address this challenge by integrating two key innovations. 1) We first propose a guidance-aware spherical interpolation design that incorporates the explicit guidance from the input images by modifying the self-attention modules, addressing identity loss, and ensuring consistent transitions throughout the generated sequences. 2) We further introduce a step-oriented motion flow that blends self-attention modules derived from each input image to achieve controlled and directional transitions that respect both input images. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods with training that is 10x~50x faster, establishing a new state-of-the-art for image morphing.",tldr:"A tuning-free method for image morphing via diffusion models.",image_path:"1.jpg"},{title:"AID: Attention Interpolation of Text-to-Image Diffusion",author:"Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao#",conference:"Neural Information Processing Systems (NeurIPS), 2024",time:"2024-09",link:{paper:"https://arxiv.org/abs/2403.17924",github:"https://github.com/QY-H00/attention-interpolation-diffusion"},abstract:"Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater consistency, smoothness, and efficiency, and offers control over the exact path of interpolation. Our approach demonstrates effectiveness for conceptual and spatial interpolation.",tldr:"A training-free method for text-to-image diffusion models to generate interpolation between different conditions.",image_path:"2.jpg",bibtex:"@article{he2024aid,\n  title={AID: Attention Interpolation of Text-to-Image Diffusion},\n  author={He, Qiyuan and Wang, Jinghao and Liu, Ziwei and Yao, Angela},\n  journal={arXiv preprint arXiv:2403.17924},\n  year={2024}\n}"},{title:"Pair then Relation: Pair-Net for Panoptic Scene Graph Generation",author:"Jinghao Wang*, Zhengyu Wen*, Xiangtai Li, Zujing Guo, Jingkang Yang, Ziwei Liu#",conference:"IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024",time:"2024-08",link:{paper:"https://arxiv.org/abs/2307.08699",github:"https://github.com/king159/Pair-Net"},abstract:"Panoptic Scene Graph (PSG) is a challenging task in Scene Graph Generation (SGG) that aims to create a more comprehensive scene graph representation using panoptic segmentation instead of boxes. However, current PSG methods have limited performance, which can hinder downstream task development. To improve PSG methods, we conducted an in-depth analysis to identify the bottleneck of the current PSG models, finding that inter-object pair-wise recall is a crucial factor which was ignored by previous PSG methods. Based on this, we present a novel framework: Pair then Relation (Pair-Net), which uses a Pair Proposal Network (PPN) to learn and filter sparse pair-wise relationships between subjects and objects. We also observed the sparse nature of object pairs and used this insight to design a lightweight Matrix Learner within the PPN. Through extensive ablation and analysis, our approach significantly improves upon leveraging the strong segmenter baseline. Notably, our approach achieves new state-of-the-art results on the PSG benchmark, with over 10% absolute gains compared to PSGFormer.",tldr:"A SOTA and lightweight model for panoptic scene graph generation.",image_path:"3.jpg",bibtex:"@article{wang2024pair,\n  title={Pair then relation: Pair-net for panoptic scene graph generation},\n  author={Wang, Jinghao and Wen, Zhengyu and Li, Xiangtai and Guo, Zujin and Yang, Jingkang and Liu, Ziwei},\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  year={2024},\n  publisher={IEEE}\n}"},{title:"Mimic-it: Multi-modal in-context instruction tuning",author:"Bo Li*, Yuanhan Zhang*, Liangyu Chen*, Jinghao Wang*, Fanyi Pu*, Joshua Adrian Cahyono, Jingkang Yang, Chunyuan Li, Ziwei Liu#",conference:"IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024, under review",time:"2023-06",link:{paper:"https://arxiv.org/abs/2306.05425",github:"https://github.com/Luodian/Otter"},abstract:"Recent advances in Large Multimodal Models (LMMs) have unveiled great potential as visual assistants. However, most existing works focus on responding to individual instructions or using previous dialogues for contextual understanding. There is little discussion on employing both images and text as in-context examples to enhance the instruction following capability. To bridge this gap, we introduce the Otter model to leverage both textual and visual in-context examples for instruction tuning. Specifically, Otter builds upon Flamingo with Perceiver architecture, and has been instruction tuned for general purpose multi-modal assistant. Otter seamlessly processes multi-modal inputs, supporting modalities including text, multiple images, and dynamic video content. To support the training of Otter, we present the MIMIC-IT (MultI-Modal In-Context Instruction Tuning) dataset, which encompasses over 3 million multi-modal instruction-response pairs, including approximately 2.2 million unique instructions across a broad spectrum of images and videos. MIMIC-IT has been carefully curated to feature a diverse array of in-context examples for each entry. Comprehensive evaluations suggest that instruction tuning with these in-context examples substantially enhances model convergence and generalization capabilities. Notably, the extensive scenario coverage provided by the MIMIC-IT dataset empowers the Otter model to excel in tasks involving complex video and multi-image understanding.",tldr:"A multi-model dataset and model for in-context instruction tuning.",image_path:"4.jpeg",bibtex:"@article{li2023mimic,\n  title={Mimic-it: Multi-modal in-context instruction tuning},\n  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},\n  journal={arXiv preprint arXiv:2306.05425},\n  year={2023}\n}"},{title:"TransPatch: A Transformer-based Generator for Accelerating Transferable Patch Generation in Adversarial Attacks Against Object Detection Models",author:"Jinghao Wang, Chenling Cui, Xuejun Wen#, Jie Shi",conference:"European Conference on Computer Vision Workshop on Adversarial Robustness in the Real World (ECCV-W), 2022",time:"2023-02",link:{paper:"https://link.springer.com/chapter/10.1007/978-3-031-25056-9_21"},abstract:"Patch-based adversarial attack shows the possibility to black-box physical attacks on state-of-the-art object detection models through hiding the occurrence of the objects, which causes a high risk in automated security system relying on such model. However, most prior works mainly focus on the attack performance but rarely pay attention to the training speed due to pixel updating and non-smoothing loss function in the training process. To overcome this limitation, we propose a simple but novel training pipeline called TransPatch, a transformer-based generator with new loss function, to accelerate the training process. To address the issue of unstable training problem of previous methods, we also compare and visualize the landscape of various loss functions. We conduct comprehensive experiments on two pedestrian and one stop sign datasets on three object detection models, i.e., YOLOv4, DETR and SSD to compare the training speed and patch performance in such adversarial attacks. From our experiments, our method outperforms previous methods within the first few epochs, and achieves absolute 20% ~ 30% improvements in attack success rate (ASR) using 10% of the training time. We hope our approach can motivate future research on using generator in physical adversarial attack generation on other tasks and models.",tldr:"A generator for patch-based adversarial attacks on object detection models.",image_path:"5.jpg",bibtex:"@inproceedings{wang2022transpatch,\n  title={TransPatch: a transformer-based generator for accelerating transferable patch generation in adversarial attacks against object detection models},\n  author={Wang, Jinghao and Cui, Chenling and Wen, Xuejun and Shi, Jie},\n  booktitle={European Conference on Computer Vision},\n  pages={317--331},\n  year={2022},\n  organization={Springer}\n}\n"}],X=t(4467),N=t(9379),B=t(2258),_=t(1822),H={All:"All",FirstAuthor:"First Author",Published:"Published/Accepted",CurrentYear:"Since ".concat((new Date).getFullYear()),Journal:"Journal Only",Conference:"Conference Only"};function q(e){var n=e.filters,t=e.setFilters,i=e.countDic,a=function(e){var i=e.target,a=i.value,o=i.checked;if("All"===a){var r=Object.values(n).reduce((function(e,n){return e+n}),0);if(1==r&&n.showAll)return;t({showAll:o,showFirstAuthor:!1,showPublished:!1,showCurrentYear:!1,showJournal:!1,showConference:!1})}else t((function(e){var n=(0,N.A)((0,N.A)({},e),{},(0,X.A)({showAll:!1},"show".concat(a.replace(" ","")),o));"Journal"===a&&o?n.showConference=!1:"Conference"===a&&o&&(n.showJournal=!1);var t=Object.values(n).reduce((function(e,n){return e+n}),0);return 0==t&&(n.showAll=!0),n}))};return(0,J.jsx)(B.A,{children:(0,J.jsx)(_.A,{row:!0,children:Object.keys(H).map((function(e){return(0,J.jsx)(s.A,{label:"".concat(H[e]," (").concat(i[e],")"),control:(0,J.jsx)(c.A,{value:e,checked:n["show".concat(e)],onChange:a,sx:{color:"#a0a0a0cc"}})},e)}))})})}function Q(e,n){var t=n.showFirstAuthor,i=n.showPublished,a=n.showCurrentYear,o=n.showJournal,r=n.showConference,s=n.showAll,c=e.author.startsWith("Jinghao Wang")||e.author.startsWith("Jinghao Wang*"),l=!("arXiv"===e.conference||e.conference.includes("under review")),u=e.time.split("-")[0]===(new Date).getFullYear().toString(),h=e.conference.includes("TPAMI"),d=!e.conference.includes("TPAMI");return s||(!t||c)&&(!i||l)&&(!a||u)&&(!o||h)&&(!r||d)}var V,U={FirstAuthor:0,Published:0,CurrentYear:0,Journal:0,Conference:0,All:R.length},K=(0,a.A)(R);try{for(K.s();!(V=K.n()).done;){var $=V.value;$.author.startsWith("Jinghao Wang")&&(U.FirstAuthor+=1),$.conference.includes("arXiv")||$.conference.includes("under review")||(U.Published+=1),$.time.split("-")[0]===(new Date).getFullYear().toString()&&(U.CurrentYear+=1),$.conference.includes("TPAMI")&&(U.Journal+=1),$.conference.includes("TPAMI")||(U.Conference+=1)}}catch(ne){K.e(ne)}finally{K.f()}function ee(){var e=(0,o.useState)({showFirstAuthor:!1,showPublished:!1,showCurrentYear:!1,showAll:!0,showJournal:!1,showConference:!1}),n=(0,i.A)(e,2),t=n[0],a=n[1],g=(0,o.useState)(!1),m=(0,i.A)(g,2),f=m[0],x=m[1];return(0,J.jsxs)(p.A,{title:"Publication",children:[(0,J.jsx)(r.A,{sx:{mt:4,mb:2},variant:"h4",color:"black",children:"Publication"}),(0,J.jsx)(q,{filters:t,setFilters:a,countDic:U}),(0,J.jsx)(s.A,{label:f?"Collapse Abstract":"Expand Abstract",control:(0,J.jsx)(c.A,{onChange:function(){x(!f)},sx:{color:"#a0a0a0cc","&.Mui-checked":{color:l.A[800]}},icon:(0,J.jsx)(h.A,{}),checkedIcon:(0,J.jsx)(u.A,{})})}),0===R.filter((function(e){return Q(e,t)})).length&&(0,J.jsx)("div",{children:(0,J.jsx)(d.A,{in:!0,timeout:500,unmountOnExit:!0,children:(0,J.jsx)(r.A,{sx:{mt:4,mb:4,display:"flex",justifyContent:"center"},variant:"h6",color:"black",children:"No publication to show"})})}),R.map((function(e){return(0,J.jsx)(d.A,{in:Q(e,t),timeout:500,unmountOnExit:!0,children:(0,J.jsx)("div",{children:(0,J.jsx)(Z,{data:e,expandAllAbstract:f})})},e.title)}))]})}}}]);
//# sourceMappingURL=798.dff7c6eb.chunk.js.map