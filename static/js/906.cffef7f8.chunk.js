"use strict";(self.webpackChunkpersonal_site=self.webpackChunkpersonal_site||[]).push([[906],{6906:(e,t,n)=>{n.r(t),n.d(t,{default:()=>f});var i=n(5043),a=n(2582),o=n(7645),s=n(3217),r=n(1906),c=n(5678),l=n(6191),h=n(6446),d=n(579);const p=e=>{let{data:t}=e;const[n,a]=(0,i.useState)(!1);return(0,d.jsx)("div",{className:"cell-container",children:(0,d.jsxs)("article",{className:"mini-post",children:[(0,d.jsx)("p",{children:(0,d.jsxs)("h3",{children:[" ",t.title]})}),(0,d.jsx)("p",{children:t.author.split(",").map(((e,n)=>{const i=e.trim(),a="Jinghao Wang"===i||"Jinghao Wang*"===i,o=i.includes("#");return(0,d.jsxs)("span",{children:[a?(0,d.jsx)("b",{children:i}):o?(0,d.jsxs)(d.Fragment,{children:[i.replace("#",""),(0,d.jsx)("sup",{children:"\u2709"})]}):i,t.author.split(",").length===n+1?"":",\xa0\xa0"]},n)}))}),(0,d.jsx)("p",{className:"conference",children:t.conference}),t.link.code?(0,d.jsxs)(h.A,{sx:{"& button":{m:1}},children:[(0,d.jsx)(r.A,{size:"small",endIcon:(0,d.jsx)(c.A,{}),onClick:()=>window.open(t.link.paper),children:"paper"}),(0,d.jsx)(r.A,{size:"small",endIcon:(0,d.jsx)(l.A,{}),onClick:()=>window.open(t.link.code),children:"code"})]}):(0,d.jsx)("p",{children:(0,d.jsx)(r.A,{size:"small",endIcon:(0,d.jsx)(c.A,{}),onClick:()=>window.open(t.link.paper),children:"paper"})}),(0,d.jsx)("p",{className:"conference",children:t.time}),n?(0,d.jsx)("p",{children:t.abstract}):(0,d.jsx)("p",{}),(0,d.jsx)(r.A,{size:"small",onClick:()=>a(!n),children:n?"Hide":"Show Abstract"})]})})},u=[{title:"AID: Attention Interpolation of Text-to-Image Diffusion",author:"Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao#",conference:"arXiv",time:"2024-03",link:{paper:"https://arxiv.org/abs/2403.17924",code:"https://github.com/QY-H00/attention-interpolation-diffusion"},abstract:"Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater consistency, smoothness, and efficiency, and offers control over the exact path of interpolation. Our approach demonstrates effectiveness for conceptual and spatial interpolation.",ref:"aid"},{title:"Pair then Relation: Pair-Net for Panoptic Scene Graph Generation",author:"Jinghao Wang*, Zhengyu Wen*, Xiangtai Li, Jingkang Yang, Zujing Guo, Ziwei Liu#",conference:"IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), under review",time:"2023-07",link:{paper:"https://arxiv.org/abs/2307.08699",code:"https://github.com/king159/Pair-Net"},abstract:"Panoptic Scene Graph (PSG) is a challenging task in Scene Graph Generation (SGG) that aims to create a more comprehensive scene graph representation using panoptic segmentation instead of boxes. However, current PSG methods have limited performance, which can hinder downstream task development. To improve PSG methods, we conducted an in-depth analysis to identify the bottleneck of the current PSG models, finding that inter-object pair-wise recall is a crucial factor which was ignored by previous PSG methods. Based on this, we present a novel framework: Pair then Relation (Pair-Net), which uses a Pair Proposal Network (PPN) to learn and filter sparse pair-wise relationships between subjects and objects. We also observed the sparse nature of object pairs and used this insight to design a lightweight Matrix Learner within the PPN. Through extensive ablation and analysis, our approach significantly improves upon leveraging the strong segmenter baseline. Notably, our approach achieves new state-of-the-art results on the PSG benchmark, with over 10% absolute gains compared to PSGFormer.",ref:"pairnet"},{title:"MIMIC-IT: Multi-Modal In-Context Instruction Tuning",author:"Bo Li*, Yuanhan Zhang*, Liangyu Chen*, Jinghao Wang*, Fanyi Pu*, Jingkang Yang, Ziwei Liu#",conference:"arXiv",time:"2023-06",link:{paper:"https://arxiv.org/abs/2306.05425",code:"https://github.com/Luodian/Otter"},abstract:"High-quality instructions and responses are essential for the zero-shot performance of large language models on interactive natural language tasks. For interactive vision-language tasks involving intricate visual scenes, a large quantity of diverse and creative instruction-response pairs should be imperative to tune vision-language models (VLMs). Nevertheless, the current availability of vision-language instruction-response pairs in terms of quantity, diversity, and creativity remains limited, posing challenges to the generalization of interactive VLMs. Here we present MultI-Modal In-Context Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal instruction-response pairs, with 2.2 million unique instructions derived from images and videos. Each pair is accompanied by multi-modal in-context information, forming conversational contexts aimed at empowering VLMs in perception, reasoning, and planning. The instruction-response collection process, dubbed as Syphus, is scaled using an automatic annotation pipeline that combines human expertise with GPT's capabilities. Using the MIMIC-IT dataset, we train a large VLM named Otter. Based on extensive evaluations conducted on vision-language benchmarks, it has been observed that Otter demonstrates remarkable proficiency in multi-modal perception, reasoning, and in-context learning. Human evaluation reveals it effectively aligns with the user's intentions. We release the MIMIC-IT dataset, instruction-response collection pipeline, benchmarks, and the Otter model.",ref:"mimicit"},{title:"Otter: A Multi-Modal Model with In-Context Instruction Tuning",author:"Bo Li*, Yuanhan Zhang*, Liangyu Chen*, Jinghao Wang*, Jingkang Yang, Ziwei Liu#",conference:"arXiv",time:"2023-05",link:{paper:"https://arxiv.org/abs/2305.03726",code:"https://github.com/Luodian/Otter"},abstract:"Large Language Models (LLMs) have exhibited exceptional universal aptitude as few/zero-shot learners for numerous tasks, thanks to their pre-training on large-scale text data. GPT-3 is a prominent LLM that has showcased significant capabilities in this regard. Furthermore, variants of GPT-3, namely InstructGPT and ChatGPT, equipped with instruction tuning, have proven effective in interpreting natural language instructions to perform complex real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo's implementation for researchers, democratizing the required training resources from 1*A100 GPU to 4*RTX-3090 GPUs, and integrate both OpenFlamingo and Otter into Hugging Face Transformers for more researchers to incorporate the models into their customized training and inference pipelines.",ref:"otter"},{title:"TransPatch: A Transformer-based Generator for Accelerating Transferable Patch Generation in Adversarial Attacks Against Object Detection Models",author:"Jinghao Wang, Chenling Cui, Xuejun Wen#, Jie Shi",conference:"European Conference on Computer Vision (ECCV) Workshop on Adversarial Robustness in the Real World (AROW), 2022",time:"2022-08",link:{paper:"https://link.springer.com/chapter/10.1007/978-3-031-25056-9_21"},abstract:"Patch-based adversarial attack shows the possibility to black-box physical attacks on state-of-the-art object detection models through hiding the occurrence of the objects, which causes a high risk in automated security system relying on such model. However, most prior works mainly focus on the attack performance but rarely pay attention to the training speed due to pixel updating and non-smoothing loss function in the training process. To overcome this limitation, we propose a simple but novel training pipeline called TransPatch, a transformer-based generator with new loss function, to accelerate the training process. To address the issue of unstable training problem of previous methods, we also compare and visualize the landscape of various loss functions. We conduct comprehensive experiments on two pedestrian and one stop sign datasets on three object detection models, i.e., YOLOv4, DETR and SSD to compare the training speed and patch performance in such adversarial attacks. From our experiments, our method outperforms previous methods within the first few epochs, and achieves absolute 20% ~ 30% improvements in attack success rate (ASR) using 10% of the training time. We hope our approach can motivate future research on using generator in physical adversarial attack generation on other tasks and models.",ref:"transpatch"}];var g=n(5553);function m(e,t,n,i,a){return(!t||((s=e.author).startsWith("Jinghao Wang")||s.startsWith("Jinghao Wang*")))&&(!n||!("arXiv"==(o=e.conference)||o.includes("under review")))&&(!i||e.time.split("-")[0]===(new Date).getFullYear().toString())||a;var o,s}function f(){const[e,t]=i.useState(!1),[n,r]=i.useState(!1),[c,l]=i.useState(!1),[h,f]=i.useState(!0);return(0,d.jsx)(o.A,{title:"Publication",children:(0,d.jsxs)("article",{className:"post",id:"projects",children:[(0,d.jsx)("header",{children:(0,d.jsx)("div",{className:"title",children:(0,d.jsx)("h2",{"data-testid":"heading",children:(0,d.jsx)(a.N_,{to:"/publication",children:"Publication"})})})}),(0,d.jsx)("div",{children:"Filter:"}),(0,d.jsx)(g.A,{label:"All",control:(0,d.jsx)(s.A,{defaultChecked:!0,checked:h,onChange:e=>{f(e.target.checked),t(!1),r(!1),l(!1)}})}),(0,d.jsx)(g.A,{label:"First Author",control:(0,d.jsx)(s.A,{checked:e,onChange:e=>{t(e.target.checked),f(!1)}})}),(0,d.jsx)(g.A,{label:"Published/Accepted",control:(0,d.jsx)(s.A,{checked:n,onChange:e=>{r(e.target.checked),f(!1)}})}),(0,d.jsx)(g.A,{label:"Since ".concat((new Date).getFullYear()),control:(0,d.jsx)(s.A,{checked:c,onChange:e=>{l(e.target.checked),f(!1)}})}),u.map((t=>m(t,e,n,c,h)&&(0,d.jsx)(p,{data:t},t.title))),0===u.filter((t=>m(t,e,n,c,h))).length&&(0,d.jsx)("p",{children:"No publication to show"})]})})}}}]);
//# sourceMappingURL=906.cffef7f8.chunk.js.map